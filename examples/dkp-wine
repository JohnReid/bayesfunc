#!/usr/bin/env python

import logging
from pathlib import Path
import numpy as np
from scipy import stats
from sklearn.model_selection import train_test_split
from sklearn.metrics import balanced_accuracy_score
from sklearn.datasets import load_wine
from sklearn.preprocessing import StandardScaler
from sklearn import svm
import torch as t
import torch.nn as nn
from torch.distributions import Categorical
import bayesfunc as bf


def calculate_elbo(net, samples):
    logits, logpq, _ = bf.propagate(net, X_train.expand(samples, -1, -1))
    loglik = Categorical(logits=logits).log_prob(y_train).sum(-1)
    assert loglik.shape == (samples,)
    assert logpq.shape == (samples,)
    return loglik + logpq / X_train.shape[0]


def train(net, samples=10):
    opt = t.optim.Adam(net.parameters(), lr=0.05)
    for i in range(2000):
        opt.zero_grad()
        elbo = calculate_elbo(net, samples=samples)
        (-elbo.mean()).backward()
        opt.step()


logging.basicConfig(level=logging.INFO)
_logger = logging.getLogger('kernel-predict-words')

expt_dir = Path('experimental') / 'one_shot'
kernel_dir = expt_dir / 'output' / 'one-shot-anchor'
output_dir = expt_dir / 'output' / 'word-predictions'
output_dir.mkdir(exist_ok=True, parents=True)

# Load data
data = load_wine()
X = data.data
y = data.target
nclasses = len(np.unique(y))
X.shape

# Standardise
scaler = StandardScaler().fit(X)
scaler.transform(X).mean(axis=0)
scaler.transform(X).std(axis=0)

# Train-test split
dtype = t.float64
device = 'cpu'
X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(scaler.transform(X), y, test_size=0.4, random_state=0)
X_train = t.from_numpy(X_train_np).to(dtype=dtype, device=device)
X_test = t.from_numpy(X_test_np).to(dtype=dtype, device=device)
y_train = t.from_numpy(y_train_np).to(dtype=int, device=device)
y_test = t.from_numpy(y_test_np).to(dtype=int, device=device)

# Define network
inducing_batch = 137
net = nn.Sequential(
    bf.FeaturesToKernel(inducing_batch=inducing_batch),
    bf.SqExpKernelGram(),
    bf.IWLayer(inducing_batch),
    bf.SqExpKernelGram(),
    bf.GIGP(out_features=nclasses, inducing_batch=inducing_batch)
)
net = bf.InducingWrapper(net, inducing_batch=inducing_batch, inducing_shape=(inducing_batch, X.shape[-1]))
net = net.to(device=device, dtype=dtype)

_logger.info('Training')
inducing_data_orig = net[0].inducing_data.detach().numpy().copy()
train(net)
inducing_data_post = net[0].inducing_data.detach().numpy().copy()
inducing_data_orig - inducing_data_post  # Check inducing points moved

# Predict
with t.no_grad():
    logit_pred = net(X_test.expand(100, -1, -1))
    y_dist = Categorical(logits=logit_pred)
    y_ll = y_dist.log_prob(y_test)
    y_ll.mean()
    y_sample = y_dist.sample()
    y_pred = stats.mode(y_sample.numpy()).mode[0]  # Take most often sampled as prediction
    _logger.info('Balanced accuracy: %f', balanced_accuracy_score(y_test_np, y_pred))

# Try sklearn SVM
_logger.info('Fitting SVM')
model = svm.SVC(class_weight='balanced')
model.fit(X_train_np, y_train_np)
y_svm = model.predict(X_test_np)
_logger.info('SVM balanced accuracy: %f', balanced_accuracy_score(y_test_np, y_svm))
