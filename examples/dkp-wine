#!/usr/bin/env python

import logging
from pathlib import Path
import numpy as np
from scipy import stats
import seaborn as sb
import matplotlib.pyplot as plt
from matplotlib import cm
from matplotlib import colors
from sklearn.model_selection import train_test_split
from sklearn.metrics import balanced_accuracy_score
from sklearn.datasets import load_wine
from sklearn.preprocessing import StandardScaler
from sklearn import svm
import torch as t
import torch.nn as nn
from torch.distributions import Categorical
import bayesfunc as bf


def get_activation(name):
    """Get the activations when registered as a forward hook."""
    def hook(model, input, output):
        activation[name] = output
    return hook


def calculate_elbo(net, samples):
    logits, logpq, _ = bf.propagate(net, X_train.expand(samples, -1, -1))
    loglik = Categorical(logits=logits).log_prob(y_train).sum(-1)
    assert loglik.shape == (samples,)
    assert logpq.shape == (samples,)
    return loglik + logpq / X_train.shape[0]


def train(net, samples=10):
    opt = t.optim.Adam(net.parameters(), lr=0.05)
    for i in range(2000):
        opt.zero_grad()
        elbo = calculate_elbo(net, samples=samples)
        (-elbo.mean()).backward()
        opt.step()


logging.basicConfig(level=logging.INFO)
_logger = logging.getLogger('kernel-predict-words')

expt_dir = Path('experimental') / 'one_shot'
kernel_dir = expt_dir / 'output' / 'one-shot-anchor'
output_dir = expt_dir / 'output' / 'word-predictions'
output_dir.mkdir(exist_ok=True, parents=True)

# Load data
data = load_wine()
X = data.data
y = data.target
nclasses = len(np.unique(y))
_logger.info('Have %d data', X.shape[0])
_logger.info('The data have %d features', X.shape[-1])
_logger.info('Have %d classes', nclasses)

# Standardise
scaler = StandardScaler().fit(X)
scaler.transform(X).mean(axis=0)
scaler.transform(X).std(axis=0)

# Train-test split
dtype = t.float64
device = 'cpu'
X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(scaler.transform(X), y, test_size=0.4, random_state=0)
X_train = t.from_numpy(X_train_np).to(dtype=dtype, device=device)
X_test = t.from_numpy(X_test_np).to(dtype=dtype, device=device)
y_train = t.from_numpy(y_train_np).to(dtype=int, device=device)
y_test = t.from_numpy(y_test_np).to(dtype=int, device=device)
_logger.info('Have %d training data', X_train.shape[0])
_logger.info('Have %d test data', X_test.shape[0])

# Define network
inducing_batch = 37
net = nn.Sequential(
    bf.FeaturesToKernel(inducing_batch=inducing_batch),
    bf.ReluKernelGram(),
    bf.IWLayer(inducing_batch),
    bf.ReluKernelGram(),
    bf.IWLayer(inducing_batch),
    bf.ReluKernelGram(),
    bf.GIGP(out_features=nclasses, inducing_batch=inducing_batch)
)
net = bf.InducingWrapper(net, inducing_batch=inducing_batch, inducing_shape=(inducing_batch, X.shape[-1]))
net = net.to(device=device, dtype=dtype)

_logger.info('Training')
inducing_data_orig = net[0].inducing_data.detach().numpy().copy()
train(net)
inducing_data_post = net[0].inducing_data.detach().numpy().copy()
inducing_data_orig - inducing_data_post  # Check inducing points moved

# Predict
with t.no_grad():
    logit_pred = net(X_test.expand(100, -1, -1))
    y_dist = Categorical(logits=logit_pred)
    y_ll = y_dist.log_prob(y_test)
    y_ll.mean()
    y_sample = y_dist.sample()
    y_dkp = stats.mode(y_sample.numpy()).mode[0]  # Take most often sampled as prediction
    _logger.info('DKP balanced accuracy: %f', balanced_accuracy_score(y_test_np, y_dkp))

# Try sklearn SVM
_logger.info('Fitting SVM')
model = svm.SVC(class_weight='balanced')
model.fit(X_train_np, y_train_np)
y_svm = model.predict(X_test_np)
_logger.info('SVM balanced accuracy: %f', balanced_accuracy_score(y_test_np, y_svm))

# Register hooks to access activations
activation = {}
hooks = {}
for l, layer in enumerate(net[1]):
    hooks[l] = layer.register_forward_hook(get_activation(l))

# Visualisations
_logger.info('Visualising')
with t.no_grad():

    # Forward pass
    nsamples = 3
    logit_pred = net(X_test.expand(nsamples, -1, -1))
    y_dist = Categorical(logits=logit_pred)

    # Inspect parameters
    dir(net[1][2])
    for param in net[1][2].named_parameters():
        print(param)
    net[1][2].V
    net[1][2].log_diag
    net[1][2].log_delta
    net[1][2].log_gamma

    # Gather data for plotting
    subnet = net[1]
    nlayers = len(subnet) - 1
    nlayers
    data = np.empty((nsamples, nlayers, inducing_batch, inducing_batch))
    for sample in range(nsamples):
        for layer in range(nlayers):
            data[sample, layer] = activation[layer].ii.detach()[sample]
    vmin = data.min()
    vmax = data.max()

    # Plot
    plt.close('all')
    fig, axes = plt.subplots(nrows=nsamples, ncols=nlayers + 1, figsize=(nlayers * 6, nsamples * 6))
    cmap = plt.get_cmap('coolwarm')
    norm = colors.TwoSlopeNorm(vmin=vmin, vcenter=0., vmax=vmax)
    axes.shape
    for sample in range(nsamples):
        _logger.info('Plotting sample %d', sample)
        for layer in range(nlayers):
            sb.heatmap(data=data[sample, layer], ax=axes[sample, layer], cmap=cmap, norm=norm, cbar=False)
        axes[sample, -1].axis('off')
    fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=axes[:, -1])
    fig.savefig('dkp-visualisation.png')
